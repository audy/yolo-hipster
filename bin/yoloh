#!/usr/bin/env python

import logging

import click
import pandas as pd

import pickle

from skbio.parse.sequences import parse_fasta

from sklearn.pipeline import Pipeline
from sklearn.feature_extraction.text import HashingVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.preprocessing import LabelEncoder


@click.group()
def cli():
    pass

@click.command()
@click.option('--model', help='pickled model built with yoloh train')
@click.option('--sequences', help='sequences to classify')
def classify(*args, **kwargs):

    logging.info('loading model')
    with open(kwargs['model'], 'rb') as handle:
        model = pickle.load(handle)

    logging.info('reading sequences')
    sequences = parse_fasta(kwargs['sequences'])


@click.command()
@click.option('--sequences', help='training sequences. hint: use greengenes.')
@click.option('--taxonomies', help='training taxonomies. hint: use greengenes.')
@click.option('--output', help='where to save trained model.')
@click.option('--rank', help='taxonomic rank to train at', default='Species')
@click.option('--ngram-range', nargs = 2, type=int, default=(4, 4))
def train(*args, **kwargs):

    # load taxonomies into pandas DataFrame
    # for convenience, we're gonna store sequences and labels in
    # this dataframe as well.
    logging.info('reading taxonomies from %s' % kwargs['taxonomies'])
    tax_table = pd.read_csv(kwargs['taxonomies'], dtype=str, index_col=0)
    logging.info('loaded %s taxonomic descriptions' % len(tax_table))

    # load sequences
    logging.info('reading sequences from %s' % kwargs['sequences'])
    with open(kwargs['sequences']) as handle:
        seqs = list(parse_fasta(handle))
    logging.info('loaded %s sequences' % len(seqs))

    # train
    tax_table = tax_table.dropna()
    logging.info('kept %s taxonomic descriptions' % len(tax_table))

    # 1. keep only sequences with valid classification at rank
    # 2. convert to dictionary id -> sequence
    keep_ids = set(str(i) for i in tax_table.index)
    seqs = { r[0]: r[1] for r in seqs if r[0] in keep_ids }
    logging.info('kept %s sequences' % len(seqs))

    # put sequences in tax_table
    tax_table.sequence = [ seqs[i] for i in tax_table.index ]

    logging.info('classifying at %s rank' % kwargs['rank'])

    # transform labels
    label_encoder = LabelEncoder()
    logging.info('transforming labels using %s' % label_encoder)
    tax_table.label = label_encoder.fit_transform(tax_table[kwargs['rank']])

    # define transformation and classification pipeline
    hasher = HashingVectorizer(analyzer='char',
                               ngram_range=kwargs['ngram_range'],
                               non_negative=True)

    classifier = MultinomialNB()

    pipeline = Pipeline([('transformer', hasher),
                         ('classifier', classifier)])

    # fit the classifier
    logging.info('fitting classifier')

    pipeline.fit(tax_table.sequence, tax_table.label)

    # pickle
    # (we might want to store this in some kind of struct that stores
    # metadata about the database such as kmer size, creation datetime
    # original sequences, etc...)
    logging.info('saving model to %s' % kwargs['output'])
    with open(kwargs['output'], 'wb') as handle:
        pickle.dump(pipeline.named_steps['classifier'], handle)

    logging.info('done. have a nice day')



cli.add_command(train)

if __name__ == '__main__':
    logging.basicConfig(level=logging.INFO, logfile='/dev/stderr')
    cli()
